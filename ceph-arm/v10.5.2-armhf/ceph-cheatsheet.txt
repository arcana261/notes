Remember:

1. configure data scrubbing hours
2. performance domains
3. failure domains
4. seperation of journals
5. for SSD performance domains, you can increase filestore max sync interval
6. compute journalsize from formulae {2 * (expected throughput * filestore max sync interval)}


http://ceph.com/pgcalc/

http://accelazh.github.io/ceph/Ceph-Performance-Tuning-Checklist

https://www.sebastien-han.fr/blog/2012/12/07/ceph-2-speed-storage-with-crush/

http://docs.ceph.com/docs/master/rados/operations/crush-map/

http://redhatstorage.redhat.com/2015/02/12/10-commands-every-ceph-administrator-should-know/
1. Check or watch cluster health: ceph status || ceph -w
2. Check cluster usage stats: ceph df
3. Check placement group stats: ceph pg dump
4. View the CRUSH map: ceph osd tree
5. Create or remove OSDs: ceph osd create || ceph osd rm
6. Create or delete a storage pool: ceph osd pool create || ceph osd pool delete
7. Repair an OSD: ceph osd repair
8. Benchmark an OSD: ceph tell osd.* bench
9. Adjust an OSD’s crush weight: ceph osd crush reweight
10. List cluster keys: ceph auth list
ceph osd primary-affinity <osd-id> <weight>

(weight is between [0,1])


http://cephnotes.ksperis.com/blog/2015/02/02/crushmap-example-of-a-hierarchical-cluster-map

## remove osd
###################################

# admin:
ceph osd out {osd-num}

# osd:
systemctl stop ceph-osd@{osd-num}

# admin:
ceph osd crush remove osd.{osd-num}
ceph osd crush remove osd{osd-num}
ceph auth del osd.{osd-num}
ceph osd rm {osd-num}
vim ceph.conf
.....
remove [osd.{osd-num] section if present
.....

# osd:


## set primary affinity
###################################
[mon]
...
mon osd allow primary affinity = true
...

ceph osd primary-affinity osd.<id> <weight>


NOTE: weight is between 0 and 1

## decompile crush map
###################################

ceph osd getcrushmap -o my-crush-map
crushtool -d my-crush-map -o my-crush-map.txt

## compile crush map
###################################

crushtool -c crushmap.txt -o crushmap-new.bin

## test utilization of custom crush map
###################################
crushtool --test -i crushmap-new.bin --show-utilization --rule 0 --num-rep=2
crushtool --test -i crushmap-new.bin --show-choose-tries --rule 0 --num-rep=2
crushtool --test -i test-map.bin --show-bad-mappings --rule 0 --num-rep=3

## set crushmap to the compiled version
###################################

ceph osd setcrushmap -i /tmp/crush-compiled1

## using admin socket
###################################

ceph --admin-daemon /var/run/ceph/<socket_name> help
ceph --admin-daemon /var/run/ceph/<socket_name> <admin_socket_command>

## cpeh high level monitoring
###################################

ceph health
ceph status
ceph quorum_status
ceph mon_status
## watch cluster ## ceph -w
## cluster usage statistics ## ceph df

ceph mon stat
ceph mon dump

ceph osd stat
ceph osd dump
ceph osd tree

ceph pg stat
ceph pg dump
ceph pg map <pg-num>

## PG (Placement Group) States
###################################

Creating         | Ceph is still creating the placement group.
Active           | Ceph will process requests to the placement group.
Clean            | Ceph replicated all objects in the placement group the correct number of times.
Down             | A replica with necessary data is down, so the placement group is offline.
Replay           | The placement group is waiting for clients to replay operations after an OSD crashed.
Scrubbing        | Ceph is checking the placement group for inconsistencies.
Degraded         | Ceph has not replicated some objects in the placement group the correct number of times yet.
Inconsistent     | Ceph detects inconsistencies in the one or more replicas of an object in the placement group (e.g. objects are the wrong size, objects are missing from one replica after recovery finished, etc.).
Peering          | The placement group is undergoing the peering process
Repair           | Ceph is checking the placement group and repairing any inconsistencies it finds (if possible).
Recovering       | Ceph is migrating/synchronizing objects and their replicas.
Backfill         | Ceph is scanning and synchronizing the entire contents of a placement group instead of inferring what contents need to be synchronized from the logs of recent operations. Backfill is a special case of recovery.
Wait-backfill    | The placement group is waiting in line to start backfill.
Backfill-toofull | A backfill operation is waiting because the destination OSD is over its full ratio.
Incomplete       | Ceph detects that a placement group is missing information about writes that may have occurred, or does not have any healthy copies. If you see this state, try to start any failed OSDs that may contain the needed information.
Stale            | The placement group is in an unknown state - the monitors have not received an update for it since the placement group mapping changed.
Remapped         | The placement group is temporarily mapped to a different set of OSDs from what CRUSH specified.
Undersized       | The placement group fewer copies than the configured pool replication level.
Peered           | The placement group has peered, but cannot serve client IO due to not having enough copies to reach the pool’s configured min_size parameter. Recovery may occur in this state, so the pg may heal up to min_size eventually.


NOTES:

1. when creating: "Creating -> Peering -> Active"


