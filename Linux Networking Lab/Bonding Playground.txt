Where possible 802.3ad is recommended,
its going to give you the least amount of issues.
otherwise balance-[alb,tlb] is a good choice,
followed by balance-rr if connected to a single switch
or balance-xor if connected to multiple.
Active-backup is a good choice if you are not concerned
about bandwidth but more about link failure and finally
broadcast is very situation specific and
only recommended if you know you need it.





balance-rr, active-backup, balance-tlb and balance-alb don't need switch support.

balance-rr augments performance at the price of fragmentation, performs poorly with some protocols (CIFS) and with more than 2 interfaces.

balance-alb and balance-tlb may not work properly with all switches; there are often some arp problems (some machines may fail to connect to each other for instance). You may need to tweak various settings (miimon, updelay) to get stable networking.

balance-xor may or may not require switch configuration. You need to set up an interface group (not LACP) on HP and Cisco switches, but apparently it's not necessary on D-Link, Netgear and Fujitsu switches.

802.3ad absolutely requires an LACP group on the switch side. It's the best supported option overall for augmenting performance.

Note: whatever you do, one network connection always go through one and only one physical link. So when aggregating GigE interfaces, a file transfer from machine A to machine B can't top 1 gigabit/s, even if each machine has 4 aggregated GigE interfaces (whatever the bonding mode in use).










   balance-rr or 0
       Round-robin policy: Transmit packets in sequential
       order from the first available slave through the
       last. This mode provides load balancing and fault
       tolerance. 


   active-backup or 1
       Active-backup policy: Only one slave in the bond is
       active.  A different slave becomes active if, and only
       if, the active slave fails. The bond's MAC address is
       externally visible on only one port (network adapter)
       to avoid confusing the switch.

       This mode provides fault tolerance. The "primary"
       option affects the behavior of this mode.

   balance-xor or 2
       XOR policy: Transmit based on the selected transmit
       hash policy.  The default policy is a simple [(source
       MAC address XOR'd with destination MAC address) modulo
       slave count].  Alternate transmit policies may be
       selected via the xmit_hash_policy option.

       This mode provides load balancing and fault tolerance.

   broadcast or 3
       Broadcast policy: transmits everything on all slave
       interfaces.  This mode provides fault tolerance.

   802.3ad or 4
       IEEE 802.3ad Dynamic link aggregation.  Creates
       aggregation groups that share the same speed and
       duplex settings.  Utilizes all slaves in the active
       aggregator according to the 802.3ad specification.

       Slave selection for outgoing traffic is done according
       to the transmit hash policy, which may be changed from
       the default simple XOR policy via the xmit_hash_policy
       option. Note that not all transmit policies may be 802.3ad
       compliant, particularly inregards to the packet mis-ordering
       requirements of section 43.2.4 of the 802.3ad standard.
       Differing peer implementations will have varying tolerances for
       noncompliance.

       Note: Most switches will require some type of configuration
       to enable 802.3ad mode.

   balance-tlb or 5
       Adaptive transmit load balancing: channel bonding that
       does not require any special switch support.  The
       outgoing traffic is distributed according to the
       current load (computed relative to the speed) on each
       slave.  Incoming traffic is received by the current
       slave.  If the receiving slave fails, another slave
       takes over the MAC address of the failed receiving
       slave.

   balance-alb or 6
       Adaptive load balancing: includes balance-tlb plus
       receive load balancing (rlb) for IPV4 traffic, and
       does not require any special switch support.

       When a link is reconnected or a new slave joins the
       bond the receive traffic is redistributed among all
       active slaves in the bond by initiating ARP Replies
       with the selected MAC address to each of the
       clients. The updelay parameter must
       be set to a value equal or greater than the switch's
       forwarding delay so that the ARP Replies sent to the
       peers will not be blocked by the switch.






--------       ------------
|      |       |          |
|  S1  |<----->|  bridge  |<-----> S2
|      |<----->|          |
|      |<----->|          |<-----> S3
|      |<----->|          |
|      |       |          |<-----> S4
--------       ------------

NOTE: the actual topology is a little more complex
to allow creation of virtual delay and 10Mbit/s fake
bandwidth on veths's.

-----------------------------------------------------------------------
|  Bonding Mode   |  Single    |  Multiple  |  Single    |  Multiple  |
|                 |  Outgoing  |  Outgoing  |  Incoming  |  Incoming  |
|-----------------|------------|------------|------------|------------|
|  balance-rr     |  37.8      |  5.61      |  37.8      |  11.0      |
|                 |            |  10.9      |            |  11.0      |
|                 |            |  10.9      |            |  16.1      |
|-----------------|------------|------------|------------|------------|
|  balance-xor    |  10.8      |  5.84      |  10.8      |  6.18      |
|                 |            |  10.7      |            |  5.74      |
|                 |            |  5.75      |            |  10.1      |
|-----------------|------------|------------|------------|------------|
|  balance-tlb    |  10.9      |  10.9      |  10.9      |  4.02      |
|                 |            |  10.4      |            |  3.77      |
|                 |            |  10.4      |            |  3.78      |
|-----------------|------------|------------|------------|------------|
|  balance-alb    |  10.6      |  10.8      |  10.9      |  10.9      |
|                 |            |  10.9      |            |  9.78      |
|                 |            |  10.8      |            |  9.91      |
|-----------------|------------|------------|------------|------------|
|  active-backup  |  10.8      |  3.78      |  10.9      |  4.02      |
|                 |            |  3.75      |            |  3.76      |
|                 |            |  3.77      |            |  3.78      |
|-----------------|------------|------------|------------|------------|
|  brodcast       |  x         |  x         |  x         |  x         |
|-----------------|------------|------------|------------|------------|
|  802.3ad        |  N/A       |  N/A       |  N/A       |  N/A       |
-----------------------------------------------------------------------

-----------------------------------------------------------------------
|  Bonding Mode   |  Single    |  Multiple  |  Single    |  Multiple  |
|                 |  Outgoing  |  Outgoing  |  Incoming  |  Incoming  |
|-----------------|------------|------------|------------|------------|
|  balance-rr     |  YES(*,**) |  YES(*,**) |  YES(**)   |  YES(**)   |
|-----------------|------------|------------|------------|------------|
|  balance-xor    |  NO        |  YES(*,**) |  NO        |  YES(*,**) |
|-----------------|------------|------------|------------|------------|
|  balance-tlb    |  NO        |  YES       |  NO        |  NO        |
|-----------------|------------|------------|------------|------------|
|  balance-alb    |  NO        |  YES       |  NO        |  YES       |
|-----------------|------------|------------|------------|------------|
|  active-backup  |  NO        |  NO        |  NO        |  NO        |
|-----------------|------------|------------|------------|------------|
|  brodcast       |  x         |  x         |  x         |  x         |
|-----------------|------------|------------|------------|------------|
|  802.3ad        |  N/A       |  N/A       |  N/A       |  N/A       |
-----------------------------------------------------------------------

YES(*): Not using all available bandwidth, but partially
YES(**): Unbalanced traffic rate between links

######################################################
## load bonding kernel module
######################################################

# load kernel module
sudo modprobe bonding

######################################################
## setup network interfaces
######################################################

sudo ip link add name br0 type bridge
sudo ip link set dev br0 up

sudo ip netns add server1

for i in $(seq 0 3); do \
	sudo ip link add name br-delay-${i} type bridge; \
	sudo ip link set dev br-delay-${i} up; \
	sudo ip link add name br-speed-${i} type bridge; \
	sudo ip link set dev br-speed-${i} up; \
	sudo ip link add name veth-s1-${i} type veth peer name peer-s1-${i}; \
	sudo ip link set dev veth-s1-${i} up; \
	sudo ip link set dev veth-s1-${i} master br-delay-${i}; \
	sudo ip link set dev peer-s1-${i} netns server1; \
	sudo ip netns exec server1 ip link set dev peer-s1-${i} name eth${i}; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
	sudo ip link add name veth-s1-${i}-delay type veth peer name peer-s1-${i}-delay; \
	sudo ip link set dev veth-s1-${i}-delay up; \
	sudo ip link set dev peer-s1-${i}-delay up; \
	sudo ip link set dev veth-s1-${i}-delay master br-delay-${i}; \
	sudo ip link set dev peer-s1-${i}-delay master br-speed-${i}; \
	sudo tc qdisc add dev veth-s1-${i}-delay root netem delay 50ms; \
	sudo tc qdisc add dev peer-s1-${i}-delay root netem delay 50ms; \
	sudo ip link add name veth-s1-${i}-speed type veth peer name peer-s1-${i}-speed; \
	sudo ip link set dev veth-s1-${i}-speed up; \
	sudo ip link set dev peer-s1-${i}-speed up; \
	sudo ip link set dev veth-s1-${i}-speed master br-speed-${i}; \
	sudo ip link set dev peer-s1-${i}-speed master br0; \
	sudo tc qdisc add dev veth-s1-${i}-speed root handle 1: htb default 10; \
	sudo tc class add dev veth-s1-${i}-speed parent 1: classid 1:1 htb rate 10mbit burst 15k; \
	sudo tc class add dev veth-s1-${i}-speed parent 1:1 classid 1:10 htb rate 10mbit burst 15k; \
	sudo tc qdisc add dev veth-s1-${i}-speed parent 1:10 handle 10: sfq perturb 10; \
	sudo tc filter add dev veth-s1-${i}-speed parent 1: protocol ip prio 1 u32 match ip dst 192.168.80.1/32 flowid 1:10; \
	sudo tc filter add dev veth-s1-${i}-speed parent 1: protocol ip prio 2 u32 match ip dst 192.168.80.2/32 flowid 1:10; \
	sudo tc filter add dev veth-s1-${i}-speed parent 1: protocol ip prio 3 u32 match ip dst 192.168.80.3/32 flowid 1:10; \
	sudo tc filter add dev veth-s1-${i}-speed parent 1: protocol ip prio 4 u32 match ip dst 192.168.80.4/32 flowid 1:10; \
	sudo tc qdisc add dev peer-s1-${i}-speed root handle 1: htb default 10; \
	sudo tc class add dev peer-s1-${i}-speed parent 1: classid 1:1 htb rate 10mbit burst 15k; \
	sudo tc class add dev peer-s1-${i}-speed parent 1:1 classid 1:10 htb rate 10mbit burst 15k; \
	sudo tc qdisc add dev peer-s1-${i}-speed parent 1:10 handle 10: sfq perturb 10; \
	sudo tc filter add dev peer-s1-${i}-speed parent 1: protocol ip prio 1 u32 match ip dst 192.168.80.1/32 flowid 1:10; \
	sudo tc filter add dev peer-s1-${i}-speed parent 1: protocol ip prio 2 u32 match ip dst 192.168.80.2/32 flowid 1:10; \
	sudo tc filter add dev peer-s1-${i}-speed parent 1: protocol ip prio 3 u32 match ip dst 192.168.80.3/32 flowid 1:10; \
	sudo tc filter add dev peer-s1-${i}-speed parent 1: protocol ip prio 4 u32 match ip dst 192.168.80.4/32 flowid 1:10; \
done

for i in $(seq 2 4); do \
	sudo ip netns add server${i}; \
	sudo ip link add name veth-s${i} type veth peer name peer-s${i}; \
	sudo ip link set dev veth-s${i} up; \
	sudo ip link set dev veth-s${i} master br0; \
	sudo ip link set dev peer-s${i} netns server${i}; \
	sudo ip netns exec server${i} ip link set dev peer-s${i} name eth0; \
	sudo ip netns exec server${i} ip link set dev eth0 up; \
	sudo ip netns exec server${i} ip addr add 192.168.80.${i}/24 brd + dev eth0; \
done

######################################################
## setup iperf server on server2, server3, server4
######################################################

sudo ip netns exec server2 iperf -s
sudo ip netns exec server3 iperf -s
sudo ip netns exec server4 iperf -s

######################################################
## test connectivity from server1
######################################################

# test single streams
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev eth0
sudo ip netns exec server1 ping -c 2 192.168.80.2
sudo ip netns exec server1 ping -c 2 192.168.80.3
sudo ip netns exec server1 ping -c 2 192.168.80.4
sudo ip netns exec server1 iperf -c 192.168.80.2
sudo ip netns exec server1 iperf -c 192.168.80.3
sudo ip netns exec server1 iperf -c 192.168.80.4
sudo ip netns exec server1 ip addr flush eth0

# test multiple stream
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev eth0
sudo ip netns exec server1 iperf -c 192.168.80.2&
sudo ip netns exec server1 iperf -c 192.168.80.3&
sudo ip netns exec server1 iperf -c 192.168.80.4&

# cleanup
sudo ip netns exec server1 ip addr flush eth0

######################################################
## setup balance-rr bonding
######################################################
balance-rr: Send packets out each interface sequentially.
While this does give fault tolerance, if using only one switch
it will only give Aggregate bandwidth for packets leaving
via this interface. All incoming packets will be limited to the
speed of the slowest link in the bond. Also keep in mind that
this will cause the ARP entries for the machine to bounce between
interfaces rapidly in the single switch situation. 
Connecting to multiple upstream switches
(one for each link in the bond) will not suffer from these affects
and should receive maximum bandwidth on
both egressing and ingressing traffic.


Outgoing to 1 Server: Aggregated
Outgoing to N Servers: Aggregated
Incoming from 1 Server: NOT Aggregated
Incoming from N Servers: Aggregated
Fault Tolerance: Tolerant

######################################################


# setup bonding
sudo ip netns exec server1 ip link add name bond0 type bond
sudo ip netns exec server1 bash -c "echo 'balance-rr' > /sys/class/net/bond0/bonding/mode"
for i in $(seq 0 3); do \
	sudo ip netns exec server1 ip addr flush eth${i}; \
	sudo ip netns exec server1 ip link set dev eth${i} down; \
	sudo ip netns exec server1 ip link set dev eth${i} master bond0; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link set dev bond0 up
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev bond0
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/mode"
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/slaves"

# test connectivity
sudo ip netns exec server1 ping -c 2 192.168.80.2
sudo ip netns exec server1 ping -c 2 192.168.80.3
sudo ip netns exec server1 ping -c 2 192.168.80.4

# test single outgoing stream
sudo ip netns exec server1 iperf -c 192.168.80.2

# test multiple outgoing streams
sudo ip netns exec server1 iperf -c 192.168.80.2&
sudo ip netns exec server1 iperf -c 192.168.80.3&
sudo ip netns exec server1 iperf -c 192.168.80.4&

# setup iperf server on server1
sudo ip netns exec server1 iperf -s

# test single incoming stream
sudo ip netns exec server2 iperf -c 192.168.80.1

# test multiple incoming streams
sudo ip netns exec server2 iperf -c 192.168.80.1&
sudo ip netns exec server3 iperf -c 192.168.80.1&
sudo ip netns exec server4 iperf -c 192.168.80.1&

######### SET DEIVCES DOWN ONE-BY-ONE IN SERVER1 AND REPEAT TESTS!
### sudo ip link set dev veth-s1-0 down
### sudo ip link set dev veth-s1-1 down
### sudo ip link set dev veth-s1-2 down
### sudo ip link set dev veth-s1-3 down

# destroy bonding
for i in $(seq 0 3); do \
	sudo ip link set dev veth-s1-${i} up; \
	sudo ip netns exec server1 ip link set dev eth${i} nomaster; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link delete bond0

######################################################
## setup balance-tlb bonding
######################################################
balance-tlb: Attempts to balance outgoing traffic evenly
over all interfaces and requires no special switch support.
Provides Load balancing and Aggregate bandwidth.

######################################################

# setup bonding
sudo ip netns exec server1 ip link add name bond0 type bond
sudo ip netns exec server1 bash -c "echo 'balance-tlb' > /sys/class/net/bond0/bonding/mode"
for i in $(seq 0 3); do \
	sudo ip netns exec server1 ip addr flush eth${i}; \
	sudo ip netns exec server1 ip link set dev eth${i} down; \
	sudo ip netns exec server1 ip link set dev eth${i} master bond0; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link set dev bond0 up
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev bond0
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/mode"
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/slaves"

######################################################
## setup balance-alb bonding
######################################################
balance-alb: Same as balance-tlb but also actively updates
the MAC address table of the switch at the other end by
using ARP requests to attempt to load balance incoming traffic
(The difference between the two is that in balance-tlb,
the MAC entries are passively updated by packets egressing
the interface while this transmits packets to actively update them).

######################################################

# setup bonding
sudo ip netns exec server1 ip link add name bond0 type bond
sudo ip netns exec server1 bash -c "echo 'balance-alb' > /sys/class/net/bond0/bonding/mode"
for i in $(seq 0 3); do \
	sudo ip netns exec server1 ip addr flush eth${i}; \
	sudo ip netns exec server1 ip link set dev eth${i} down; \
	sudo ip netns exec server1 ip link set dev eth${i} master bond0; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link set dev bond0 up
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev bond0
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/mode"
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/slaves"

######################################################
## setup balance-xor bonding
######################################################
balance-xor: Send packets out based on a configurable
policy of XORing important filed in the packet before transmission.
The exact policy can be set with the xmit_hash_policy
(consult the Linux documentation for more info).
One advantage of this mode is that packets to the same host
will be sent out the same interface which may produce
more predictable traffic patterns.
Provides Outgoing aggregate bandwidth and Fault Tolerance.

layer2:
Uses XOR of hardware MAC addresses to generate the hash.
This algorithm will place all traffic to a particular network peer on the same slave.

layer2+3:
Uses XOR of hardware MAC addresses and IP addresses to generate the hash.
This algorithm will place all traffic to a particular network peer on the same slave.

layer3+4:
This policy uses upper layer protocol information, when available,
to generate the hash. This allows for traffic to a particular network
peer to span multiple slaves,
although a single connection will not span multiple slaves.

encap2+3:
This policy uses the same formula as layer2+3 but it relies on
skb_flow_dissect to obtain the header fields which might result
in the use of inner headers if an encapsulation protocol is used.

encap3+4:
This policy uses the same formula as layer3+4 but it relies on
skb_flow_dissect to obtain the header fields which might result
in the use of inner headers if an encapsulation protocol is used. 

######################################################

# setup bonding
sudo ip netns exec server1 ip link add name bond0 type bond
sudo ip netns exec server1 bash -c "echo 'balance-xor' > /sys/class/net/bond0/bonding/mode"
sudo ip netns exec server1 bash -c "echo 'layer2' > /sys/class/net/bond0/bonding/xmit_hash_policy"
for i in $(seq 0 3); do \
	sudo ip netns exec server1 ip addr flush eth${i}; \
	sudo ip netns exec server1 ip link set dev eth${i} down; \
	sudo ip netns exec server1 ip link set dev eth${i} master bond0; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link set dev bond0 up
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev bond0
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/mode"
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/slaves"
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/xmit_hash_policy"

######################################################
## setup active-backup bonding
######################################################
active-backup: Only one link in the bond will be used
at a time with the primary being chosen by the name
of the interface in /sys/class/net/<interface>/bonding/primary,
updating the interface in this file will change the
primary (active) interface.
This mode only provides fault tolerance.

######################################################

# setup bonding
sudo ip netns exec server1 ip link add name bond0 type bond
sudo ip netns exec server1 bash -c "echo 'active-backup' > /sys/class/net/bond0/bonding/mode"
for i in $(seq 0 3); do \
	sudo ip netns exec server1 ip addr flush eth${i}; \
	sudo ip netns exec server1 ip link set dev eth${i} down; \
	sudo ip netns exec server1 ip link set dev eth${i} master bond0; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link set dev bond0 up
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev bond0
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/mode"
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/slaves"

######################################################
## setup broadcast bonding
######################################################
broadcast: All traffic going out is broadcast on all links,
most useful in situations where you have 2 mirrors of your
network that are not connected to each other (fully isolated).
This mode provides fault tolerance.

######################################################

# setup bonding
sudo ip netns exec server1 ip link add name bond0 type bond
sudo ip netns exec server1 bash -c "echo 'broadcast' > /sys/class/net/bond0/bonding/mode"
for i in $(seq 0 3); do \
	sudo ip netns exec server1 ip addr flush eth${i}; \
	sudo ip netns exec server1 ip link set dev eth${i} down; \
	sudo ip netns exec server1 ip link set dev eth${i} master bond0; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link set dev bond0 up
sudo ip netns exec server1 ip addr add 192.168.80.1/24 brd + dev bond0
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/mode"
sudo ip netns exec server1 bash -c "cat /sys/class/net/bond0/bonding/slaves"

################ CLEANUP #################

for i in $(seq 0 3); do \
	sudo ip link set dev veth-s1-${i} up; \
	sudo ip netns exec server1 ip link set dev eth${i} nomaster; \
	sudo ip netns exec server1 ip link set dev eth${i} up; \
done
sudo ip netns exec server1 ip link delete bond0

for i in $(seq 1 4); do \
	sudo ip netns delete server${i}; \
done

for i in $(seq 2 4); do \
	sudo ip link delete veth-s${i}; \
done

for i in $(seq 0 3); do \
	sudo ip link delete br-delay-${i}; \
	sudo ip link delete br-speed-${i}; \
	sudo ip link delete veth-s1-${i}; \
	sudo ip link delete veth-s1-${i}-delay; \
	sudo ip link delete veth-s1-${i}-speed; \
done

sudo ip link delete br0


